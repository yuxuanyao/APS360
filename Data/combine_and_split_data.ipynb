{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13681, 2)\n",
      "(35887, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mux = pd.read_csv(\"./MuxspaceDataset/data/new_legend.csv\", usecols=[\"emotion\", \"image\"])\n",
    "fer = pd.read_csv(\"./KaggleFERDataset/icml_face_data.csv\", usecols=[\"emotion\", \" pixels\"])\n",
    "\n",
    "print(mux.shape)\n",
    "print(fer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49568, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = pd.DataFrame(columns=[\"emotion\", \"image\",\" pixels\"])\n",
    "combined = combined.append(mux)\n",
    "combined = combined.append(fer)\n",
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    14685\n",
       "6    13066\n",
       "4     6345\n",
       "0     5205\n",
       "2     5142\n",
       "5     4370\n",
       "1      755\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined[\"emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3696, 3)\n",
      "(791, 3)\n",
      "(798, 3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "To ensure equal number of samples, use 755 as total\n",
    "\"\"\"\n",
    "emotions = [combined[combined.emotion == 0], combined[combined.emotion == 1], combined[combined.emotion == 2], \n",
    "           combined[combined.emotion == 3], combined[combined.emotion == 4], combined[combined.emotion == 5],\n",
    "           combined[combined.emotion == 6]]\n",
    "\n",
    "training = pd.DataFrame(columns=[\"emotion\", \"image\",\" pixels\"])\n",
    "validation = pd.DataFrame(columns=[\"emotion\", \"image\",\" pixels\"])\n",
    "test = pd.DataFrame(columns=[\"emotion\", \"image\",\" pixels\"])\n",
    "\n",
    "for emotion in emotions:\n",
    "    sample = emotion.sample(n=755, random_state=1)\n",
    "    training = training.append(sample[:int(0.7*sample.shape[0])])\n",
    "    valid_and_test = sample[int(0.7*sample.shape[0]):]\n",
    "    validation = validation.append(valid_and_test[:int(0.5*valid_and_test.shape[0])])\n",
    "    test = test.append(valid_and_test[int(0.5*valid_and_test.shape[0]):])\n",
    "\n",
    "print(training.shape)\n",
    "print(validation.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.to_csv('./training.csv',index=False)\n",
    "validation.to_csv('./validation.csv', index=False)\n",
    "test.to_csv('./test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=48x48 at 0x1E3134BC520>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The following are examples for applying transformations to pixel values:\n",
    "1. Resize image\n",
    "\"\"\"\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "image_pixels = fer.iloc[0][\" pixels\"]\n",
    "image_string = image_pixels.split(' ')\n",
    "image_data = np.asarray(image_string, dtype=np.uint8).reshape(48, 48)\n",
    "Image.fromarray(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=48x48 at 0x1E3134BC3A0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "2. Flip image along vertical axis\n",
    "\"\"\"\n",
    "Image.fromarray(np.fliplr(image_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load image and label to dataloader\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "train_data = []\n",
    "print(len(image_data))\n",
    "for i in range(len(image_data)):\n",
    "    train_data.append([image_data[i], \"label\"])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=100)\n",
    "image, label = next(iter(trainloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
